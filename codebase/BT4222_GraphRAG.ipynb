{"cells":[{"cell_type":"markdown","metadata":{"id":"Onfdg461rDOr"},"source":["<H1>Graph Retreval Augmented Generation with LangChain and Neo4j</H1>\n","\n","Questions are provided at the end of the notebook (4 marks)\n","\n","Submission Due Date: November 1 2024\n","\n","\n","**Versioning**\n","- Revised by Alvern Ong Wei Zhe in August 2024\n","- Revised by Wang Qiuhong in September-Octorber 2024\n","\n","**Consultation**\n","- RAG and neo4j: Li Kaixin <likaixin@u.nus.edu>\n","- Assignment: Goh Kaitlyn Wen Jing <e0774226@u.nus.edu>"]},{"cell_type":"markdown","metadata":{"id":"YSFJEJtLrDOu"},"source":["<img src=\"https://drive.google.com/uc?id=1y0rfDpTbkPr32UkP9KMY_01Ff2shCc6H\" width=\"500\" height=\"300\"></img>\n","\n","In this jupyter notebook, we will be walking through how to implement a Large Language Model (LLM) with Retrevial Augemented Generation (RAG).\n","\n","The notebook will be broken down into 3 main steps and cover two forms of RAG - one with graphs and the other without. In step 3a, we will be defining a RAG model without graphs to show an example of how to implement a standard RAG model. In step 3b, we will focus on how to implement graphs in the RAG structure with neo4j. In this step, we will be creating a hybrid retriever that is a combination of a structred retriever and unstructured retriever.\n","\n","A unstructured retriever will retreive data from text sources. This data can be in the form of text passages, vector embeddings, etc.\n","\n","A structured retriever will retreive data from structured data sources such as databases, tables or knowledge graphs.\n","\n","This hybrid retriever will then parse the data from the structured and unstructured retriever into the LLM and the LLM will produce a response.\n","\n","<h3><u>TechStack:</u></h3>\n","\n","**LangChain**\n","- A framework designed to help developers build and integrate applications that use large language models (LLMs) and other AI models\n","- For example, we will use the following functions to customize a prompt:\n"," - **ChatPromptTemplate.from_template**: from a template with placeholders for dynamic values\n"," - **ChatPromptTemplate.from_message**: from a list of structured message objects\n","\n","**Neo4j**\n","- A graph databse management system used to manage and query graph data\n","- For example, we will use the following functions in Python to communicate with a Neo4j instance\n"," - **Neo4jGraph.add_graph_documents** to import graph data into the database\n"," - **Neo4jGraph.query** to retrive information from the neo4j graph database\n","\n","<h3><u>The sections are :</u></h3>\n","<ol>\n","    <li>Section 1: Initialisation</li>\n","    <li>Section 2: Load data for RAG</li>\n","    <li>Section 3a: Defining a RAG structure without graphs</li>\n","    <li>Section 3b: Defining a RAG structure with graphs</li>\n","    <ol>\n","        <li> Step 3b.1: Adding documents to the graph structure</li>\n","        <li> Step 3b.2: Creating a Hybrid Retrevial for RAG</li>\n","            <ol>\n","                <li> Step 3b.2.1: Unstructured data retriever</li>\n","                <li> Step 3b.2.2: Structured data retriever</li>\n","                <li> Step 3b.2.3: Hybrid data retriever</li>\n","            </ol>\n","        <li> Step 3b.3 Define the RAG chain</li>\n","    </ol>\n","</ol>\n","\n","\n","Reference\n","- https://medium.com/@jinglemind.dev/mastering-advanced-rag-methods-graphrag-with-neo4j-implementation-with-langchain-42b8f1d05246"]},{"cell_type":"markdown","metadata":{"id":"mi4-59_srDOv"},"source":["<h2>Download required modules through pip </h2>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNp51PyDrDOw","collapsed":true},"outputs":[],"source":["%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs py2neo faiss-cpu pypdf"]},{"cell_type":"code","source":["%pip install unstructured[all-docs]"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0obGRE504f0_","outputId":"99dfea8d-3a4a-41ac-d01e-d857d22fa5da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unstructured[all-docs]\n","  Downloading unstructured-0.15.13-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n","Collecting filetype (from unstructured[all-docs])\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured[all-docs])\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n","Collecting emoji (from unstructured[all-docs])\n","  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.7)\n","Collecting python-iso639 (from unstructured[all-docs])\n","  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n","Collecting langdetect (from unstructured[all-docs])\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.26.4)\n","Collecting rapidfuzz (from unstructured[all-docs])\n","  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting backoff (from unstructured[all-docs])\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n","Collecting unstructured-client (from unstructured[all-docs])\n","  Downloading unstructured_client-0.26.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n","Collecting python-oxmsg (from unstructured[all-docs])\n","  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n","Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n","  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Collecting pikepdf (from unstructured[all-docs])\n","  Downloading pikepdf-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n","Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.2)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7)\n","Collecting google-cloud-vision (from unstructured[all-docs])\n","  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n","Collecting unstructured-inference==0.7.36 (from unstructured[all-docs])\n","  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n","Collecting pdf2image (from unstructured[all-docs])\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting onnx (from unstructured[all-docs])\n","  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting pypandoc (from unstructured[all-docs])\n","  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n","Collecting pdfminer.six (from unstructured[all-docs])\n","  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n","Collecting pi-heif (from unstructured[all-docs])\n","  Downloading pi_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n","Collecting effdet (from unstructured[all-docs])\n","  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.0.1)\n","Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n","Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.24.7)\n","Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.10.0.84)\n","Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (3.7.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (2.4.1+cu121)\n","Collecting timm (from unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.44.2)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (10.4.0)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n","  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (24.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.22.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.19.1+cu121)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n","Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n","  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.19.2)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[all-docs]) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2024.9.11)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.2)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (43.0.1)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n","Collecting olefile (from python-oxmsg->unstructured[all-docs])\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2024.8.30)\n","Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (0.2.0)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (0.27.2)\n","Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured[all-docs])\n","  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n","Requirement already satisfied: pydantic<2.10.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (2.9.2)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.65.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.64.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n","Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (24.3.25)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client->unstructured[all-docs]) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client->unstructured[all-docs]) (2.23.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[all-docs]) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.16.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2024.6.1)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36->unstructured[all-docs]) (0.19.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.1)\n","Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.1.5)\n","Collecting pdfminer.six (from unstructured[all-docs])\n","  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n","Downloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Downloading pi_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (984 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.8/984.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pikepdf-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypandoc-1.14-py3-none-any.whl (21 kB)\n","Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.15.13-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.26.0-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n","Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Building wheels for collected packages: langdetect, antlr4-python3-runtime, iopath\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=540c9aa56d6f5ae43844bd73dfb48a860e8a03f2832cdbb96d5d1bb503d11f3a\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=41d0b021f86ecf63985ebe304f7ef85e66ef9c9a035d6bf3470cd25abed894df\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=5dde1de72ce1d0e707454f9ec0434fd66e3a1b6e56271dbef103888186d1a7b1\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built langdetect antlr4-python3-runtime iopath\n","Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pypdfium2, pypandoc, portalocker, pi-heif, pdf2image, onnx, omegaconf, olefile, langdetect, jsonpath-python, humanfriendly, emoji, backoff, python-pptx, python-oxmsg, pikepdf, iopath, coloredlogs, unstructured-client, pdfminer.six, onnxruntime, unstructured, timm, pdfplumber, layoutparser, google-cloud-vision, effdet, unstructured-inference\n","Successfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 effdet-0.4.1 emoji-2.14.0 filetype-1.2.0 google-cloud-vision-3.7.4 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 olefile-0.47 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.19.2 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pi-heif-0.18.0 pikepdf-9.3.0 portalocker-2.10.1 pypandoc-1.14 pypdfium2-4.30.0 python-docx-1.1.2 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.12 python-oxmsg-0.0.1 python-pptx-1.0.2 rapidfuzz-3.10.0 timm-1.0.9 unstructured-0.15.13 unstructured-client-0.26.0 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.13\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","pydevd_plugins"]},"id":"d9d45ffa380a4f0bbc8c2d5e180df58a"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"CvNEJJ3srDOx"},"source":["<h2>Import all required modules</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKUfZBRBrDOx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"39b2f348-3225-4f1b-9580-121fc85c08c2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n","\n","For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n","with: `from pydantic import BaseModel`\n","or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["# LangChain's core runnables for orchestrating tasks in workflows\n","from langchain_core.runnables import (\n","    RunnableBranch,\n","    RunnableLambda,\n","    RunnableParallel,\n","    RunnablePassthrough,\n",")\n","# LangChain's core components for building custom prompts, handling messages, and parsing outputs\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.prompts.prompt import PromptTemplate\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Typing Imports\n","from typing import Tuple, List\n","\n","# Integrating LangChain with Neo4j, which can be useful for tasks like combining graph databases and vector stores for advanced AI workflows.\n","# For example:\n","# We can use Neo4jGraph to retrieve structured graph data from Neo4j\n","# We can store and query document embeddings using Neo4jVector\n","# We can leverage LLMGraphTransformer to help the LLM reason about relationships within the graph\n","# We can use remove_lucene_chars to ensure that queries passed into Neo4j are well-formatted and don’t cause issues with search.\n","from langchain_community.graphs import Neo4jGraph\n","from langchain_community.vectorstores import Neo4jVector\n","from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n","from langchain_experimental.graph_transformers import LLMGraphTransformer\n","\n","# Document Loaders and Text Splitters\n","# from langchain.document_loaders import WikipediaLoader\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain.text_splitter import TokenTextSplitter\n","\n","# LangChain components that interface with OpenAI models\n","# ChatOpenAI handles interactive conversations with a language model\n","# OpenAIEmbeddings transform text into vectors, stores and compares the semantic meaning of user inputs or documents in a vector store like Neo4jVector.\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","\n","# Neo4j & Graph Visualization\n","# To establish a connection with a Neo4j database and handling the graph database by running Cypher queries, interacting with nodes and relationships\n","from neo4j import GraphDatabase\n","# To visually represent the graph data retrieved from Neo4j\n","from yfiles_jupyter_graphs import GraphWidget\n","\n","# FAISS (Facebook AI Similarity Search) stores text embeddings and then retrieves similar documents based on a query\n","from langchain.vectorstores import FAISS\n","\n","# Chains for QA by combining a retrieval mechanism (like FAISS) with a language model\n","from langchain.chains import RetrievalQA\n","\n","# Miscellaneous\n","import os\n","import warnings\n","import textwrap\n","\n","#colab imports if running in Google colab\n","try:\n","  import google.colab\n","  from google.colab import output\n","  output.enable_custom_widget_manager()\n","except:\n","  pass\n","\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"markdown","metadata":{"id":"pssuxLaWrDOy"},"source":["#<h1>Section 1. Initialisation</h1>"]},{"cell_type":"markdown","metadata":{"id":"7xMgCZU8rDOy"},"source":["##<h2>Step 1.1: Initialise a Neo4j Database Instance.</h2>\n","<h3>Create an instance using Neo4j Aura. (launched through a web browser)</h3>\n","\n","Neo4j is the graph database application we will use to store the Graph documents.\n","\n","<ol>\n","    <li>Create an account with Neo4j Aura and log in to the web browser.</li>\n","    <li>Click 'New Instance' under the 'Instances' tab and select the 'Free' option.\n","        <ul>\n","            <li>Remember to save the generated password as it will be needed to access your db instance.</li>\n","        </ul>\n","    </li>\n","    <li>Look for your Connection URI in the instance and save it as well</li>\n","    <li>Lastly, key in your OpenAI API Key, your username ('neo4j' by default), your db password (Step 2) and URI (Step 3) into the field below.\n","    <li>Note: if you created a free neo4j instance but did not run it for a while, you need to resume it on the neo4j web portal, which may take a few minutes as it is an on-demand service.\n","</ol>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVuEnNt2rDOy"},"outputs":[],"source":["#Save these variables in your environment\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-5SE5-0_NCW9JO_sUsI4K8lP1f6n5L5UW_XXbikGKhqT3BlbkFJxLymQhYjHR6VKPNM2XRzejPcMuAVWIJyJkr0ykj1YA\"\n","os.environ[\"NEO4J_URI\"] = 'neo4j+s://ca0da43f.databases.neo4j.io'\n","os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n","os.environ[\"NEO4J_PASSWORD\"] = 'AvNscVmezenv0ZJ4aefrS0bG-cRN0EL4nAnDuhqLE2Q'\n","\n","# Create a connection to the Neo4j database\n","# graph = Neo4jGraph()\n","graph = Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"]) # Explicitly pass the connection details to Neo4jGraph"]},{"cell_type":"markdown","metadata":{"id":"aR7WBfD_rDOz"},"source":["##<h2>Step 1.2: Initialise LLM</h2>\n","\n","Load the large language model that has already been trained by OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY22zQ9IrDOz"},"outputs":[],"source":["#Initialize the Language Model and Graph Transformer\n","llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\") # gpt-4-0125-preview occasionally has issues but in theory you would want to use the most capable model to construct the graph\n","llm_transformer = LLMGraphTransformer(llm=llm)"]},{"cell_type":"markdown","metadata":{"id":"iZDSMnpQrDOz"},"source":["## Step 1.3: An example on how the LLM will respond to a prompt which it has no knowledge on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7LQ4JhSrDOz","outputId":"204311b2-8eea-4345-889f-b4dc341130f3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of LLM without RAG process: \n","\n","The deliverables of BT4222, a module in the National University of Singapore's\n","School of Computing, typically include individual or group projects,\n","presentations, reports, and possibly exams.   Students are assessed through a\n","combination of these deliverables, with their performance being evaluated based\n","on criteria such as the quality of their work, their understanding of the\n","subject matter, their ability to apply concepts to real-world problems, and\n","their communication and presentation skills. Grades are usually assigned based\n","on a combination of these factors, with a weighting assigned to each deliverable\n","based on its importance in the overall assessment of the module.\n"]}],"source":["print('Example of LLM without RAG process: \\n')\n","response = llm(\"What is the deliverables of the BT4222, and how are students assessed?\").content\n","wrapped_response = textwrap.fill(response, width=80)\n","print(wrapped_response)\n"]},{"cell_type":"markdown","metadata":{"id":"B6lScHTXrDO0"},"source":["As you can see from the output of the llm, the answer is unrelated to what we are actually trying to ask the large language model. This is because the llm was not trained on the information we are trying to retreive. Thus, we can use retreival augmented generation to feed the model relevant information before an answer is generated."]},{"cell_type":"markdown","metadata":{"id":"Sdu2iosRrDO0"},"source":["#<h1> Section 2. Load data for RAG </h1>"]},{"cell_type":"markdown","metadata":{"id":"lrS9_Zz8rDO0"},"source":["<h2>Load Data</h2>\n","\n","For this demonstration, we will use information from the BT4222 course project deliverable guideline pdf file. We can utilize LangChain loaders to fetch and split the documents from PDFs seamlessly."]},{"cell_type":"code","source":["from google.colab import drive\n","### Permit this notebook to access your Google Drive files?\n","### Please select \"Connect to Google Drive\", choose your account and select continue\n","drive.mount('/content/drive')"],"metadata":{"id":"IAzSfJDBXgPq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"739c129e-ca4e-4401-e7ef-c32846f24d83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["### adjust the data directory if needed\n","### For example, here we created a \"data\" folder\n","### in Google Drive and put the data files needed under the \"data\" folder\n","%cd /content/drive/My Drive/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2wm6vcvXp9p","outputId":"166e93ba-3313-449f-b6e8-77e005395ba2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/data\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVBShsvtrDO0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"50214a9f-8bb3-479d-e679-869d554208bc"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 33 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 51 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 56 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 58 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 70 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 77 0 (offset 0)\n","WARNING:pypdf._reader:Ignoring wrong pointing object 84 0 (offset 0)\n"]},{"output_type":"stream","name":"stdout","text":["5\n"]}],"source":["#Load the relevant data using the pyPDFLoader\n","pdf_loader = PyPDFLoader(\"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf\")\n","raw_documents = pdf_loader.load()\n","print(len(raw_documents))\n"]},{"cell_type":"code","source":["# Define chunking strategy\n","text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=20)\n","documents = text_splitter.split_documents(raw_documents)"],"metadata":{"id":"3HhRlFPaIWB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"The number of chunks from the raw documents:\",len(documents))\n","# Print each key-value pair from the __dict__ attribute line by line\n","print(\"\\n Attributes of documents[0]:\\n\")\n","for key, value in documents[2].__dict__.items():\n","    print(f\"{key}: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyMZDNbZ6pNJ","outputId":"484afd02-b350-41e0-d504-c8d814c2b352"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of chunks from the raw documents: 7\n","\n"," Attributes of documents[0]:\n","\n","id: None\n","metadata: {'source': 'Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf', 'page': 2}\n","page_content: ©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2\n","type: Document\n"]}]},{"cell_type":"markdown","metadata":{"id":"vcNxoec4rDO1"},"source":["#<h1> Section 3a. Defining a RAG structure without graphs </h1>"]},{"cell_type":"markdown","metadata":{"id":"A_EkTJ0LrDO1"},"source":["<img src=\"https://drive.google.com/uc?id=1d97tQa3yuMupwvWRuGeeUi-PqOe4ZdHZ\" width=\"400\"></img>\n","\n","In this section, we will demonstrate how to set up a standard RAG structure without the graph database."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqyavzuqrDO1"},"outputs":[],"source":["# Initializes the embeddings model from OpenAI. This model converts text into numerical vectors.\n","embeddings = OpenAIEmbeddings()\n","\n","# Uses the FAISS library to create a vector store from the documents. FAISS is a library for efficient similarity search.\n","# It indexes the documents after converting them to vectors using the embeddings model, allowing for fast retrieval.\n","vectorstore = FAISS.from_documents(documents, embeddings)"]},{"cell_type":"code","source":["# Print basic information\n","print(f\"Type of vectorstore: {type(vectorstore)}\")\n","print(f\"Number of documents: {len(vectorstore.index_to_docstore_id)}\")\n","\n","# Print information about the underlying FAISS index\n","faiss_index = vectorstore.index\n","print(f\"\\nFAISS Index type: {type(faiss_index)}\")\n","print(f\"FAISS Index dimension: {faiss_index.d}\")\n","print(f\"Total number of vectors: {faiss_index.ntotal}\")\n","\n","\n","# Print some example document IDs\n","print(\"\\nExample document IDs:\")\n","for i, doc_id in list(vectorstore.index_to_docstore_id.items())[:len(vectorstore.index_to_docstore_id)]:\n","    print(f\"Index {i}: Document ID {doc_id}\")\n","\n","\n","print('The last two vector embeddings stored in vectorstore:\\n')\n","vectors = vectorstore.index.reconstruct_n(len(vectorstore.index_to_docstore_id)-2, 2)\n","print(vectors)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKaCDGU9YcX5","outputId":"f7ece5af-8cd8-47a4-b588-f307fc608c8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of vectorstore: <class 'langchain_community.vectorstores.faiss.FAISS'>\n","Number of documents: 7\n","\n","FAISS Index type: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n","FAISS Index dimension: 1536\n","Total number of vectors: 7\n","\n","Example document IDs:\n","Index 0: Document ID 392319f7-1cb5-40de-a7d3-a05d323379ed\n","Index 1: Document ID 763f7b88-1b3b-437d-80fd-569c616376f2\n","Index 2: Document ID 69105248-b13d-437c-8780-85c7ac724105\n","Index 3: Document ID b2ff5040-36b2-4952-827f-f39727a441e3\n","Index 4: Document ID ea31ae98-e10d-46d0-9e8a-7823f4f2ed20\n","Index 5: Document ID 3c0b89e5-a330-48a1-8fbb-4b1dbce344fa\n","Index 6: Document ID ddb8f64a-eb38-4477-93e1-9d59ff91dc79\n","The last two vector embeddings stored in vectorstore:\n","\n","[[-0.02597512  0.00025854  0.00511051 ... -0.03230054 -0.01881777\n","  -0.0346247 ]\n"," [-0.01280921 -0.00227121  0.01899435 ... -0.03567426 -0.00469538\n","  -0.04522463]]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJi5dZ16rDO1"},"outputs":[],"source":["# Set up the (Question-Answer)QA chain using the vectorstore as a retriever\n","qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRNpR3lwrDO1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7fd9777-e2e8-40e7-d297-0f7dad0aea3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Demonstration of RAG response:\n","\n","Yes, the project deliverable requires both source code and data. The source code\n","files should be uploaded by a specific deadline, and they should be self-\n","explanatory, able to run directly via Google Colab, and able to reproduce the\n","reported model performance. Additionally, the datasets should be provided with\n","accessible links, and a PDF file explaining their purpose and content should be\n","uploaded by a certain deadline as well.\n"]}],"source":["print('Demonstration of RAG response:\\n')\n","#question_step3a = 'Who teaches R in the NUS course about business analytics, what else is taught and how are students graded?'\n","question_step3a = 'Does the project deliverable require source code and data?'\n","\n","response = qa_chain.run(question_step3a)\n","wrapped_response = textwrap.fill(response, width=80)\n","print(wrapped_response)"]},{"cell_type":"markdown","metadata":{"id":"emDiQ2C5rDO1"},"source":["As you can see from the above response, the model is able to give a response that is relevant to BT4222.\n","\n","However, we realise that the response is not concise and more narrative.\n","\n","In the subsequent sections, we will show how graph RAG will help with this."]},{"cell_type":"markdown","metadata":{"id":"mzRaxe6drDO1"},"source":["#<h1> Section 3b. Defining a RAG structure with graphs </h1>"]},{"cell_type":"markdown","metadata":{"id":"htO6KVBirDO1"},"source":["##<h2>Step 3b.1 Add the documents to the graph (Neo4j)</h2>\n","\n"," Now it’s time to construct a graph based on the retrieved documents. For this purpose, we have implemented an LLMGraphTransformer module that simplifies constructing and storing a knowledge graph in a graph database.\n","\n","The LLM graph transformer returns graph documents, which can be imported to Neo4j via the `add_graph_documents` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5GvlcCwrDO2","collapsed":true},"outputs":[],"source":["# Construct a graph based on the retrieved documents.\n","# Using the LLMGraphTransformer module significantly simplifies constructing and storing a knowledge graph in a graph database.\n","graph_documents = llm_transformer.convert_to_graph_documents(documents)"]},{"cell_type":"code","source":["print(f\"count of documents:{len(graph_documents)}\")\n","print(f\"count of nodes in the first document chunk:{len(graph_documents[3].nodes)}\")\n","print(f\"count of relationships in the first document chunk:{len(graph_documents[3].relationships)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd8PkLNbgCSi","outputId":"8b0ce05e-c4b5-4711-b2c2-968148420b6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["count of documents:7\n","count of nodes in the first document chunk:13\n","count of relationships in the first document chunk:12\n"]}]},{"cell_type":"code","source":["print(\"As shown below, each of the document is split into nodes and relationships:\\n\")\n","\n","# Iterate through each item in graph_documents\n","for item in graph_documents[3].nodes:\n","    # Print details of the Node\n","    print(f\"Node ID: {item.id}\")\n","    print(f\"Node Type: {item.type}\")\n","    print(f\"Node Properties: {item.properties}\")\n","    print(\"-\" * 50)  # Separator for clarity\n","\n","for item in graph_documents[3].relationships:\n","    # Print details of the relationships\n","    print(f\"Relationship from: {item.source.id} (Type: {item.source.type})\")\n","    print(f\"  to: {item.target.id} (Type: {item.target.type})\")\n","    print(f\"Relationship Type: {item.type}\")\n","    print(f\"Relationship Properties: {item.properties}\")\n","    print(\"-\" * 50)  # Separator for clarity\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaWgDO5Bcj0u","outputId":"f12ee8a5-d79b-4044-bea8-c8ce299b2cbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["As shown below, each of the document is split into nodes and relationships:\n","\n","Node ID: Ensemble Learning\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Ml Models\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Innovative Architecture\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Pipeline\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Ml Methods\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Research Papers\n","Node Type: Research paper\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Creativity\n","Node Type: Concept\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Insights\n","Node Type: Concept\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Prediction Results\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Bias\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Gap\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Confusion\n","Node Type: Machine learning\n","Node Properties: {}\n","--------------------------------------------------\n","Node ID: Business Decision Making\n","Node Type: Business\n","Node Properties: {}\n","--------------------------------------------------\n","Relationship from: Ensemble Learning (Type: Machine learning)\n","  to: Ml Models (Type: Machine learning)\n","Relationship Type: INTEGRATE\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Ml Models (Type: Machine learning)\n","  to: Innovative Architecture (Type: Machine learning)\n","Relationship Type: DESIGN\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Ml Models (Type: Machine learning)\n","  to: Pipeline (Type: Machine learning)\n","Relationship Type: DESIGN\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Ml Models (Type: Machine learning)\n","  to: Ml Methods (Type: Machine learning)\n","Relationship Type: ADAPT\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Ml Methods (Type: Machine learning)\n","  to: Research Papers (Type: Research paper)\n","Relationship Type: PROPOSED_METHOD\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Creativity (Type: Concept)\n","  to: Insights (Type: Concept)\n","Relationship Type: ENHANCE\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Prediction Results (Type: Machine learning)\n","  to: Creativity (Type: Concept)\n","Relationship Type: EXPLAIN\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Prediction Results (Type: Machine learning)\n","  to: Insights (Type: Concept)\n","Relationship Type: EXPLAIN\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Prediction Results (Type: Machine learning)\n","  to: Bias (Type: Machine learning)\n","Relationship Type: IDENTIFY\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Bias (Type: Machine learning)\n","  to: Gap (Type: Machine learning)\n","Relationship Type: IDENTIFY\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Gap (Type: Machine learning)\n","  to: Confusion (Type: Machine learning)\n","Relationship Type: IDENTIFY\n","Relationship Properties: {}\n","--------------------------------------------------\n","Relationship from: Confusion (Type: Machine learning)\n","  to: Business Decision Making (Type: Business)\n","Relationship Type: IMPACT\n","Relationship Properties: {}\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Check if any nodes are available in the database\n","check_query = \"MATCH (n) RETURN count(n) AS node_count\"\n","result = graph.query(check_query)\n","for record in result:\n","    print(record[\"node_count\"])  # Should print 0 if the database is empty"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMiw09ykHUlh","outputId":"fa9c3593-7fb3-4e91-88cf-90fd8810eeae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["59\n"]}]},{"cell_type":"code","source":["# To create a new database, you can use Cypher query to delete all nodes and relationships\n","clear_db_query = \"\"\"\n","MATCH (n)\n","DETACH DELETE n\n","\"\"\"\n","\n","# Execute the query to clear the database\n","graph.query(clear_db_query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcYUJtZGHEF2","outputId":"731bcdd1-2e31-4314-a9f1-847e0c5d3c77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUOEQn2-rDO2"},"outputs":[],"source":["# baseEntityLabel: this parameter assigns an additional __Entity__ label to each node, enhancing indexing and query performance.\n","# include_source: this parameter links nodes to their originating documents, facilitating data traceability and context understanding.\n","graph.add_graph_documents(\n","    graph_documents,\n","    # Ensures that each entity in graph_documents is labeled with its base entity type\n","    baseEntityLabel=True,\n","    # Indicate that the source information (like the original document or context) should be included in the graph nodes or edges.\n","    include_source=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"Y4d2JwlUrDO2"},"source":["Now that we have added the graph documents to the graph, we will define a function to show Neo4j graph.\n","\n","In this function, we will be able to visualise the nodes and edges that we have added above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyww4MkVrDO2","colab":{"base_uri":"https://localhost:8080/","height":657,"referenced_widgets":["2cb0875157294be6a9d673a82ed6816e","3809b48b07e34490aaf63237df4cd32a"]},"outputId":"fbc3aa7b-c758-4311-b084-0be4dae435ff"},"outputs":[{"output_type":"display_data","data":{"text/plain":["GraphWidget(layout=Layout(height='640px', width='100%'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb0875157294be6a9d673a82ed6816e"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"}}}}}],"source":["# Cypher is the query language used for interacting with Neo4j.\n","# Here we generate a query that finds instances where either the source node or the target node contains 'data'.\n","# the query content is case sensitive.\n","default_cypher = \"MATCH (s)-[r]->(t) WHERE toLower(s.id) CONTAINS 'data' OR toLower(t.id) CONTAINS 'data' RETURN s, r, t\"\n","# You can try other query\n","# default_cypher = \"MATCH (s)-[r:IDENTIFY]->(t) RETURN s,r,t LIMIT 50\"\n","\n","# Function to display graph structure\n","def showGraph(cypher: str = default_cypher):\n","    # Create a neo4j session to run queries\n","    driver = GraphDatabase.driver(\n","        uri = os.environ[\"NEO4J_URI\"],\n","        auth = (os.environ[\"NEO4J_USERNAME\"],\n","                os.environ[\"NEO4J_PASSWORD\"]))\n","    session = driver.session()\n","    widget = GraphWidget(graph = session.run(cypher).graph())\n","    widget.node_label_mapping = 'id'\n","    return widget\n","\n","showGraph()"]},{"cell_type":"code","source":["# We want to understand entity 'Document' and its relationships with other entities\n","# We will use the text property of entity 'Document'\n","showGraph(\"MATCH p=(d:Document)-[]->() RETURN p LIMIT 25 UNION MATCH p=()-[]->(d:Document) RETURN p;\")"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":777,"referenced_widgets":["6915e97f75024830bac4fd12c8445957","000d3bc843914fb59126cde96a6df381"]},"id":"GqA_ypeKnx_f","outputId":"670a3586-225b-4240-e411-1d7d0cc6d9ff"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["GraphWidget(layout=Layout(height='760px', width='100%'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6915e97f75024830bac4fd12c8445957"}},"metadata":{"application/vnd.jupyter.widget-view+json":{"colab":{"custom_widget_manager":{"url":"https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"}}}}}]},{"cell_type":"markdown","metadata":{"id":"50F-4ipnrDO2"},"source":["##<h2>Step 3b.2 Creating a Hybrid Retrieval for RAG</h2>\n","\n","After the graph generation, we will start designing the hybrid retrevial function.\n","\n","We will use a hybrid retrieval approach that combines vector and keyword indexes with graph retrieval for RAG applications.\n","\n","<img src=\"https://drive.google.com/uc?id=1MJsLg6W8_7SOflvK4LP5-hbwHRUMWIe1\" width=\"400\"></img>\n","\n","The diagram illustrates a retrieval process beginning with a user posing a question, which is then directed to an RAG retriever.\n","\n","The hybrid retrevial process is circled in red and is a combination of a unstructured and structured retriever.\n","\n","This retriever employs keyword and vector searches to search through unstructured text data(unstructured retriever) and combines it with the information collected from the knowledge graph which employs graph search(structured retriever).\n","\n","The collected data from these sources is fed into an LLM to generate and deliver the final answer."]},{"cell_type":"markdown","metadata":{"id":"2Z9OO3oHrDO2"},"source":["###<h2>Step 3b.2.1 Unstructured data retriever</h2>"]},{"cell_type":"markdown","metadata":{"id":"VBAWnw3ErDO3"},"source":["<img src=\"https://drive.google.com/uc?id=1_90bjkrX_mSOKAZokxIeseb_XIFQqadS\" width=\"400\"></img>\n","\n","First, we start by designing the unsturctured retriever.\n","\n","In the `from_exisiting_graph()` method, we are using both keyword-based and vector-based searches and targetting nodes with the label 'Document'. Within the 'Document' node, we will extract the 'text' property in the node. We pick the 'Document' node as it has the 'text' property, which stores chunks of text which will be useful in providing context to the llm.\n","\n","The `similarity_search` method can be used to retreive the relevant documents. The 4 most similar documents are retreived."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsZTceUirDO3"},"outputs":[],"source":["vector_index = Neo4jVector.from_existing_graph(\n","    # Uses a model from OpenAI that converts text into vector embeddings which are used for vector-based search\n","    OpenAIEmbeddings(),\n","    # Search for similar words using a hybrid approach, combining both keyword-based and vector-based searches.\n","    search_type=\"hybrid\",\n","    # Only nodes with the Document label will be indexed\n","    node_label=\"Document\",\n","    # Within the node, we will return the 'text' property\n","    text_node_properties=[\"text\"],\n","    embedding_node_property=\"embedding\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hWcaQq2rDO8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97115649-3919-41d8-9ded-011f90185e1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the output of similarity search:\n","\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["\n"," this is matching result: \n","text: ©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2\n","\n"," there is no exactly matching strings in the result\n","text: ©QIUHONG WANG 2024 4 Level 2-3: Use data analyDcs methods (econometrics, network analysis, etc. what you have learned from the other courses) and addiDonal ML methods to clarify, disDnguish, idenDfy the bias of, or evaluate your ML output towards eﬀecDve business decision making.   Any aspects that are dis)nct from the above…  ObjecDve evidences are needed to suﬃciently jusDfy your self-evaluaDon.  • In your report, provide justification for your self-evaluation with necessary details and objective evidences. Without objective justification, the claimed contribution cannot be supported. For each aspect of your contribution, your justification should include but not limit itself to the following. o Necessary information for people to understand what you have actually done. o How does your approach differ from existing methods? You may cite established projects, papers, or the provided example source code and highlight the deviations from them. When using existing projects or papers as benchmarks, ensure they are widely recognized and not considered as noise in the field. o Any other objective and convincing evidence that can be reproduced from your models. o You should not claim a contribution that has not been ultimately used in generating your main results and the source code of which is not provided in your deliverables. Section 4 Reference (within one page) Reference must be comprehensive with correct information and valid links if applicable.  Assessment Rubrics (40 marks) § Ontime submission of all the deliverables following required format (1 mark) § Completion of the project that can sufficiently address your proposed machine learning problem by using appropriate approaches with reasonable performance; and provision of required Abstract, Sections 1-2 and 4 in your report systematically with clarity and simplicity (14 marks) § Source code quality: your source code should be well organized with modularity and following a systematic pipeline, with clear and friendly-readable markdown. (5 marks) § Understanding your data and source code to the extent that any member can answer questions related to your data or source code during your presentation (2 mark) § Presentation (4 marks) Your presentation should communicate the important information relevant to Section 1, 2, and 3.   § Contribution from the four aspects defined in the table above (14 marks)  Presentation evaluation: § Basic requirement (1 mark) o 10 minutes per group excluding Q&A; o Every group\n","\n"," there is no exactly matching strings in the result\n","text:  ensemble learning by straighRorwardly integraDng mulDple ML models Level 2-3: Design innovaDve architecture or pipeline or adapt ML methods, which have changed the way how learning and predicDon will be conducted.  AdopDng methods proposed by high-quality research papers is also encouraged.  Crea)vity and insights in understanding or further explaining the predic)on results and performance, in iden)fying the bias of machine learning output, the gap or confusion between ML outcome and business decision making  Level 1: describe results and explain the model performance without extra analysis or without further insights \n","\n"," there is no exactly matching strings in the result\n","text: ©QIUHONG WANG 2024 2 Requirement about your source code 1. Source code file name should be as self-explainable as possible. For example, step1_data_preprocess.ipynb, step2_classification_models.ipynb, step3_rnn_models.ipynb. 2. Source code files should be able to directly run via Google colab, including loading relevant raw datasets or intermediary datasets. If it is indeed not feasible, please provide a readme file to specify the running environment.  3. Source code files should be able to reproduce your reported model performance (reasonable deviation is acceptable). To serve this purpose, please highlight in your source code the scripts that are used to generate the reported performance. Please attach or keep such performance report in the source code for verification purpose.  4. Please provide necessary markdown to explain the purpose, function or approach of the important sections in your source code, in particular those that are relevant with your elaboration in the report. Format of the Project Final Report • Cover page: Project Topic, Group No., List of group members • Line spacing: 1.5 lines • Font: Arial, Calibri, or Times New Roman; Size 11 • Margins: Normal; Columns: One Content of the Project Final Report Your project report should include the following sections.  Abstract (within 300 words in a separate page)  • Summarize the business issue and the corresponding machine learning problem that have been addressed in this project. (please make sure the business issue and the machine learning problem are consistent with each other. Inaccuracy or mismatching between them will be penalized.) • Acknowledge the datasets that have been utilized and the established models (or source code) that have been adapted into your project. (details about the sources should be provided in reference, not here ). • Summarize in bullet points any achievement or noteworthy highlights directly attributed to your group’s efforts.   Section 1 Feature Engineering Instead of repeating the data description provided in your project interim report, please focus on updating the changes or the incremental data or features included in your projects, if necessary. You can follow the same format as in your project interim report by using tables.  Section 2 Models and Performance (within four pages) • Use diagrams to illustrate the models and/or model architecture that have been used in your project. Precise and succinct explanation should be provided if necessary. \n"]}],"source":["print('Example of the output of similarity search:\\n')\n","# By default the the method will return the top 4 most similar results.\n","# To tune this, we can add in a new parameter, k = number of results, in the similarity_search function.\n","\n","def display_matching_strings(results, query_string):\n","  \"\"\"Displays page_content only if it contains the query_string from the top 4 search results.\"\"\"\n","\n","  for doc in results[:4]:\n","      if query_string in doc.page_content:\n","          print(\"\\n this is matching result: \" + doc.page_content)\n","      else:\n","          print(\"\\n there is no exactly matching strings in the result\" + doc.page_content)\n","\n","# The similarity_search method is used to retrieve documents or nodes based on their vector similarity to a given query.\n","results = vector_index.similarity_search('Justification', k=4)\n","# Please note that as we search node labeled as \"Document\",\n","# the retrieved results could be very tedious as they are the text relevant to the query_string)\n","display_matching_strings(results, 'Justification')"]},{"cell_type":"markdown","metadata":{"id":"IYqylmWFrDO9"},"source":["###<h2>Step 3b.2.2 Structured retriever</h2>\n","\n","In this example, we will use a full-text index to identify relevant nodes and then return their direct neighborhood.\n","\n","<img src=\"https://drive.google.com/uc?id=1vhSHND4m3K_TZhEZH_IJvwZXaogESe5h\" width=\"400\"></img>\n","\n","\n","The graph retriever starts by identifying relevant entities in the input. For simplicity, we instruct the LLM to identify **deliverable**, **expectation**, **level** and **assessment**. To achieve this, we will use `with_structured_output` method.\n","\n","After the entity is detected in the users' question, we will use the entity to extract the structured information from the graph database (Neo4j database) in the form of nodes and edges. This will be done with cypher (Neo4j's native querying language) query.\n","\n","For example, if we ask the question 'What are the best practices for completing the course project to ensure satisfactory contribution?'. The entities detected in the question would be 'course project', 'best practices', 'satisfactory contribution'. Using this detected entities, we will query the graph database to return the neighbourhood of these entities in the form of nodes and edges."]},{"cell_type":"markdown","source":["##### Step 1. **Detect** specified entities in the input question"],"metadata":{"id":"W3l1DUB40lSn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QCBOu8n0GIT"},"outputs":[],"source":["# This class defines the output and prompt of the LLM\n","class Entities(BaseModel):\n","    \"\"\"Identifying information about entities.\"\"\"\n","\n","    # This line structures the output of the LLM to give a List of names.\n","    names: List[str] = Field(\n","        ...,\n","        description=\"All the course deliverable, expectation, level and assessment entities \"\n","        \"appear in the text\",\n","    )\n","\n","# Each tuple represents a message with a specific role and content\n","# that helps define how different messages should be strucutured\n","# and formatted when interacting with the llm.\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are tasked with extracting specific entities from the text. Focus on course deliverable, expectation, level and assessment\",\n","        ),\n","        (\n","            \"human\",\n","            \"Use the given format to extract information from the following\"\n","            \"input: {question}\",\n","        ),\n","    ]\n",")\n","\n","# Combine the prompt template (prompt) with the language model that specifies that\n","# the output should be structured in a particular way, specifically to extract entitites.\n","entity_chain = prompt | llm.with_structured_output(Entities)"]},{"cell_type":"code","source":[],"metadata":{"id":"Ak0ybVp8w5Ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CI2N9pTrDO9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bd10334-17e7-43c1-e8b2-e075ed4ba17a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Demonstration that the entity chain can now extract the entities from the users query:\n","\n","['course project', 'best practices', 'satisfactory contribution']\n"]}],"source":["question_step3b2_2 = 'What are the best practices for completing the course project to ensure satisfactory contribution?'\n","\n","print('Demonstration that the entity chain can now extract the entities from the users query:\\n')\n","print(entity_chain.invoke({\"question\": question_step3b2_2}).names)"]},{"cell_type":"markdown","metadata":{"id":"d090oRybrDO9"},"source":["#####Step 2. Create a full text **index** on nodes in the Neo4j database\n","\n","A full-text index search refers to a type of search in a database that allows you to find records based on text data contained within text fields (or properties) of the database entries. Unlike simple keyword searches that may only look for exact matches, full-text search enables more complex querying and more flexible retrieval of data, accommodating various text search requirements."]},{"cell_type":"code","source":["# Create a full text index on nodes with the __Entity__ label on the id parameter in neo4j.\n","# A full text index will allow efficient querying of long bodies of text\n","graph.query(\n","    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")"],"metadata":{"id":"-BkhUwMndhSX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"120fe596-612d-4879-cd1b-872b5acb82f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"UdPSmYV3gAxh"},"source":["#####Step 3. **Retrieve** the neighborhood of relevant nodes using the detected entities, from the graph database (Neo4j database)\n","Now that we can detect entities in the question, let's use a full-text index to map them to the knowledge graph. First, we need to define a full-text index and a function that will generate full-text queries that allow a bit of misspelling, which we won't go into much detail here."]},{"cell_type":"code","source":["def generate_full_text_query(input: str) -> str:\n","    \"\"\"\n","    Generate a full-text search query for a given input string.\n","\n","    This function constructs a query string suitable for a full-text search.\n","    It processes the input string by splitting it into words and appending a\n","    similarity threshold (~2 changed characters) to each word, then combines\n","    them using the AND operator. Useful for mapping entities from user questions\n","    to database values, and allows for some misspelings.\n","    \"\"\"\n","    full_text_query = \"\"\n","    words = [el for el in remove_lucene_chars(input).split() if el]\n","    for word in words[:-1]:\n","        full_text_query += f\" {word}~2 AND\"\n","    full_text_query += f\" {words[-1]}~2\"\n","    return full_text_query.strip()"],"metadata":{"id":"h1ElvrRSXt1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CnBwCPurDO9"},"outputs":[],"source":["import re\n","\n","def remove_lucene_chars(input: str) -> str:\n","    \"\"\"\n","    Remove special characters that are not allowed in Lucene queries.\n","    \"\"\"\n","    return re.sub(r'[^a-zA-Z0-9\\s]', '', input)\n","\n","def generate_full_text_query(input: str) -> str:\n","    \"\"\"\n","    Generate a full-text search query for a given input string.\n","\n","    This function constructs a query string suitable for a full-text search.\n","    It processes the input string by splitting it into words and appending a\n","    similarity threshold (~2 changed characters) to each word, then combines\n","    them using the AND operator. Useful for mapping entities from user questions\n","    to database values, and allows for some misspellings.\n","    \"\"\"\n","    full_text_query = \"\"\n","    words = [el for el in remove_lucene_chars(input).split() if el]\n","\n","    if not words:\n","        return \"\"\n","\n","    for word in words[:-1]:\n","        full_text_query += f\" {word}~2 AND\"\n","    full_text_query += f\" {words[-1]}~2\"\n","\n","    return full_text_query.strip()"]},{"cell_type":"markdown","metadata":{"id":"m4iKki4SrDO9"},"source":["Since we have generated full text indexes and can detect entities in the users question, we can start defining the structured retriever function.\n","\n","The `structured_retriever` function starts by detecting entities in the user question. Next, it iterates over the detected entities and uses a Cypher query to retrieve the neighborhood of relevant nodes."]},{"cell_type":"code","source":["def structured_retriever(question: str) -> str:\n","    \"\"\"\n","    Collects the neighborhood of entities mentioned\n","    in the question.\n","    \"\"\"\n","    result = \"\"\n","    entities = entity_chain.invoke({\"question\": question})\n","\n","    for entity in entities.names:\n","        # This Neo4j Cypher query performs a full-text search on nodes that have the required label, retrieving the top two matches\n","        # based on the search term provided. After this, the query then looks for relationships that point to or from this entity,\n","        # excluding relationships of type 'MENTIONS'.\n","        response = graph.query(\n","            \"\"\"\n","            CALL db.index.fulltext.queryNodes('entity', $query, {limit: 2})\n","            YIELD node, score\n","            WITH node\n","            MATCH (node)-[r]->(neighbor)\n","            WHERE type(r) <> 'MENTIONS'\n","            RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n","            UNION ALL\n","            MATCH (neighbor)-[r]->(node)\n","            WHERE type(r) <> 'MENTIONS'\n","            RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n","            LIMIT 50\n","            \"\"\",\n","            {\"query\": generate_full_text_query(entity)},\n","        )\n","\n","        # Append results\n","        result += \"\\n\".join([el['output'] for el in response]) + \"\\n\"\n","\n","    return result.strip()\n"],"metadata":{"id":"YGgEq_roYofk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sryl2jWwrDO-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce11c37c-5067-49bc-9ed7-620fb40d4330"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the output of a structured retriever: \n","\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n"]}],"source":["question_step3b2_2 = 'What are the best practices for completing the course project to ensure satisfactory contribution?'\n","print('Example of the output of a structured retriever: \\n')\n","print(structured_retriever(question_step3b2_2))"]},{"cell_type":"markdown","metadata":{"id":"ALnQrAUorDO-"},"source":["###<h2>Step 3b.2.3 Combine the Unstructured and Structured (Hybrid) Retriever</h2>"]},{"cell_type":"markdown","metadata":{"id":"SntjfEverDO-"},"source":["<img src=\"https://drive.google.com/uc?id=19nii0wD4UAi5LR9QZYVPP0NlErskoPlr\" width=\"400\"></img>\n","\n","Now, we'll combine the unstructured and structured retriever that has been defined above to create the final function that will be pass information to the LLM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TICs-9Q_rDO-"},"outputs":[],"source":["# Define a function to combine both structured and unstructred data defined above into a prompt to be fed to the LLM\n","def retriever(question: str):\n","    print(f\"Search query: {question}\")\n","    structured_data = structured_retriever(question)\n","    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n","    final_data = f\"\"\"Structured data:\n","{structured_data}\n","Unstructured data:\n","{\"#Document \". join(unstructured_data)}\n","    \"\"\"\n","    return final_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGi0w9XyrDO-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fc34144-1980-4dc5-9a0e-9304ee16c73d","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the output of final retriever: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["Structured data:\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Unstructured data:\n","\n","text: ©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2#Document \n","text: ©QIUHONG WANG 2024 4 Level 2-3: Use data analyDcs methods (econometrics, network analysis, etc. what you have learned from the other courses) and addiDonal ML methods to clarify, disDnguish, idenDfy the bias of, or evaluate your ML output towards eﬀecDve business decision making.   Any aspects that are dis)nct from the above…  ObjecDve evidences are needed to suﬃciently jusDfy your self-evaluaDon.  • In your report, provide justification for your self-evaluation with necessary details and objective evidences. Without objective justification, the claimed contribution cannot be supported. For each aspect of your contribution, your justification should include but not limit itself to the following. o Necessary information for people to understand what you have actually done. o How does your approach differ from existing methods? You may cite established projects, papers, or the provided example source code and highlight the deviations from them. When using existing projects or papers as benchmarks, ensure they are widely recognized and not considered as noise in the field. o Any other objective and convincing evidence that can be reproduced from your models. o You should not claim a contribution that has not been ultimately used in generating your main results and the source code of which is not provided in your deliverables. Section 4 Reference (within one page) Reference must be comprehensive with correct information and valid links if applicable.  Assessment Rubrics (40 marks) § Ontime submission of all the deliverables following required format (1 mark) § Completion of the project that can sufficiently address your proposed machine learning problem by using appropriate approaches with reasonable performance; and provision of required Abstract, Sections 1-2 and 4 in your report systematically with clarity and simplicity (14 marks) § Source code quality: your source code should be well organized with modularity and following a systematic pipeline, with clear and friendly-readable markdown. (5 marks) § Understanding your data and source code to the extent that any member can answer questions related to your data or source code during your presentation (2 mark) § Presentation (4 marks) Your presentation should communicate the important information relevant to Section 1, 2, and 3.   § Contribution from the four aspects defined in the table above (14 marks)  Presentation evaluation: § Basic requirement (1 mark) o 10 minutes per group excluding Q&A; o Every group#Document \n","text: ©QIUHONG WANG 2024 1 BT4222 Project Deliverable Guideline  August 2024 Table of Contents Deliverable List ........................................................................................................................... 1 Requirement about your source code ......................................................................................... 2 Format of the Project Final Report .............................................................................................. 2 Content of the Project Final Report ............................................................................................. 2 Abstract (within 300 words in a separate page) ........................................................................................... 2 Section 1 Feature Engineering ................................................................................................................... 2 Section 2 Models and Performance (within four pages) ............................................................................ 2 Section 3 Contribution and Justification ................................................................................................... 3 Section 4 Reference (within one page) ........................................................................................................ 4 Assessment Rubrics (40 marks) ................................................................................................... 4  Deliverable List \n"," Note:  1. Both the source code links (or file names) and dataset links should be listed in a pdf file with clear, precise, simple and short explanation about their purpose and content. This pdf file should be also uploaded into Canvas by 11:59 PM Nov. 13. 2. Late submission of source code and datasets will result in the absence of the corresponding evaluation. Deliverable Format Due Time Project Final Presentation .pptx uploaded to Canvas (we will upload your files into a laptop (with windows OS) in advance for your presentation purpose.) 11:59 PM Nov. 14 (Thur.)  Project Final Report .pdf or .docx uploaded to Canvas 11:59 PM Nov. 17 (Sun)  Project source code .ipynb files uploaded to Canvas or accessible via a public link 11:59 PM Nov. 13 (Wed)  Datasets .csv files including both raw datasets and intermediary datasets if applicable). Please provide the links via which the datasets can be accessed. Do not upload your data to Canvas. 11:59 PM Nov. 13 (Wed)  Project group peer evaluation form .doc or docx file uploaded to Canvas 11:59 PM Nov. 17 (Sun)  #Document \n","text: ©QIUHONG WANG 2024 2 Requirement about your source code 1. Source code file name should be as self-explainable as possible. For example, step1_data_preprocess.ipynb, step2_classification_models.ipynb, step3_rnn_models.ipynb. 2. Source code files should be able to directly run via Google colab, including loading relevant raw datasets or intermediary datasets. If it is indeed not feasible, please provide a readme file to specify the running environment.  3. Source code files should be able to reproduce your reported model performance (reasonable deviation is acceptable). To serve this purpose, please highlight in your source code the scripts that are used to generate the reported performance. Please attach or keep such performance report in the source code for verification purpose.  4. Please provide necessary markdown to explain the purpose, function or approach of the important sections in your source code, in particular those that are relevant with your elaboration in the report. Format of the Project Final Report • Cover page: Project Topic, Group No., List of group members • Line spacing: 1.5 lines • Font: Arial, Calibri, or Times New Roman; Size 11 • Margins: Normal; Columns: One Content of the Project Final Report Your project report should include the following sections.  Abstract (within 300 words in a separate page)  • Summarize the business issue and the corresponding machine learning problem that have been addressed in this project. (please make sure the business issue and the machine learning problem are consistent with each other. Inaccuracy or mismatching between them will be penalized.) • Acknowledge the datasets that have been utilized and the established models (or source code) that have been adapted into your project. (details about the sources should be provided in reference, not here ). • Summarize in bullet points any achievement or noteworthy highlights directly attributed to your group’s efforts.   Section 1 Feature Engineering Instead of repeating the data description provided in your project interim report, please focus on updating the changes or the incremental data or features included in your projects, if necessary. You can follow the same format as in your project interim report by using tables.  Section 2 Models and Performance (within four pages) • Use diagrams to illustrate the models and/or model architecture that have been used in your project. Precise and succinct explanation should be provided if necessary. \n","    \n"]}],"source":["question_step3b2_3 = 'What are the best practices for completing the course project to ensure satisfactory contribution?'\n","print('Example of the output of final retriever: \\n')\n","print(retriever(question_step3b2_3))"]},{"cell_type":"markdown","metadata":{"id":"gnwnT7oOrDO-"},"source":["##<h2>Step 3b.3 Define the RAG Chain</h2>\n","\n","We have successfully implemented the retrieval component of the RAG.\n","The following introduces the query rewriting section that allows conversational follow ups. Note that having conversational follow ups is not crucial for Graph RAG. However, we will add this section in for completeness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ec4RvyznrDO-"},"outputs":[],"source":["# Condense a chat history and follow-up question into a standalone question\n","_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n","in its original language.\n","Chat History:\n","{chat_history}\n","Follow Up Input: {question}\n","Standalone question:\"\"\"\n","CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n","\n","# Formats chat history to incorporate into a query for the LLM\n","def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n","    buffer = []\n","    for human, ai in chat_history:\n","        buffer.append(HumanMessage(content=human))\n","        buffer.append(AIMessage(content=ai))\n","    return buffer\n","\n","_search_query = RunnableBranch(\n","    # If input includes chat_history, we condense it with the follow-up question\n","    (\n","        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n","            run_name=\"HasChatHistoryCheck\"\n","        ),  # Condense follow-up question and chat into a standalone_question\n","        RunnablePassthrough.assign(\n","            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n","        )\n","        | CONDENSE_QUESTION_PROMPT\n","        | ChatOpenAI(temperature=0)\n","        | StrOutputParser(),\n","    ),\n","    # Else, we have no chat history, so just pass through the question\n","    RunnableLambda(lambda x : x[\"question\"]),\n",")"]},{"cell_type":"markdown","metadata":{"id":"MduhgqKmrDO-"},"source":["Now that we are done with this we will carry out a demonstration to show how the Graph Rag benefits from structured, unstructred and hybrid retreival."]},{"cell_type":"markdown","metadata":{"id":"SRaWGNJurDO-"},"source":["### Prompt Augumentation and LLM Response\n","\n","To demonstrate how the hybrid retrevial benefits from unstructured retreival and structured retreival.\n","We will demonstrate the LLM's response with:\n","<ol>\n","    <li>Unstructured Retrevial</li>\n","    <li>Structured Retrevial</li>\n","    <li>Unstructured and Structured Retrevial (Hybrid Retreival)</li>\n","</ol>\n","\n","To do this, we will compare the different responses by the LLM for each of the retrievals.\n","\n","Take note that some of the responses might change with each run of the demonstration. Therefore, the description written on markdown might not align with the output given sometimes. To combat this, you can generate a new response from the llm for that demonstration.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lYFGAalrDO_"},"outputs":[],"source":["demo_question = 'What are the best practices for completing the course project to ensure satisfactory contribution?'"]},{"cell_type":"markdown","metadata":{"id":"aoq6b-jPrDO_"},"source":["#####<h3>Demonstration 1: Unstructured Retrevial</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvFWOD8DrDO_"},"outputs":[],"source":["# Retrieval\n","def just_unstructured_retriever(question: str):\n","    print(f\"Search query: {question}\")\n","    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n","    final_data = f\"\"\"\n","Unstructured data:\n","{\"#Document \". join(unstructured_data)}\n","    \"\"\"\n","    return final_data\n","\n","# Prompt Augumentation: it instructs the model to answer a question using only the context provided.\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","Question: {question}\n","Use natural language and be concise.\n","Answer:\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n","unstructured_chain = (\n","    RunnableParallel(\n","        {\n","            \"context\": _search_query | just_unstructured_retriever,\n","            \"question\": RunnablePassthrough(),\n","        }\n","    )\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aazXcOvTrDO_","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"63a7987f-7f71-405d-9bf0-2a87156e7ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the retrieval output fed to the LLM: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["\n","Unstructured data:\n","\n","text: ©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2#Document \n","text: ©QIUHONG WANG 2024 4 Level 2-3: Use data analyDcs methods (econometrics, network analysis, etc. what you have learned from the other courses) and addiDonal ML methods to clarify, disDnguish, idenDfy the bias of, or evaluate your ML output towards eﬀecDve business decision making.   Any aspects that are dis)nct from the above…  ObjecDve evidences are needed to suﬃciently jusDfy your self-evaluaDon.  • In your report, provide justification for your self-evaluation with necessary details and objective evidences. Without objective justification, the claimed contribution cannot be supported. For each aspect of your contribution, your justification should include but not limit itself to the following. o Necessary information for people to understand what you have actually done. o How does your approach differ from existing methods? You may cite established projects, papers, or the provided example source code and highlight the deviations from them. When using existing projects or papers as benchmarks, ensure they are widely recognized and not considered as noise in the field. o Any other objective and convincing evidence that can be reproduced from your models. o You should not claim a contribution that has not been ultimately used in generating your main results and the source code of which is not provided in your deliverables. Section 4 Reference (within one page) Reference must be comprehensive with correct information and valid links if applicable.  Assessment Rubrics (40 marks) § Ontime submission of all the deliverables following required format (1 mark) § Completion of the project that can sufficiently address your proposed machine learning problem by using appropriate approaches with reasonable performance; and provision of required Abstract, Sections 1-2 and 4 in your report systematically with clarity and simplicity (14 marks) § Source code quality: your source code should be well organized with modularity and following a systematic pipeline, with clear and friendly-readable markdown. (5 marks) § Understanding your data and source code to the extent that any member can answer questions related to your data or source code during your presentation (2 mark) § Presentation (4 marks) Your presentation should communicate the important information relevant to Section 1, 2, and 3.   § Contribution from the four aspects defined in the table above (14 marks)  Presentation evaluation: § Basic requirement (1 mark) o 10 minutes per group excluding Q&A; o Every group#Document \n","text: ©QIUHONG WANG 2024 1 BT4222 Project Deliverable Guideline  August 2024 Table of Contents Deliverable List ........................................................................................................................... 1 Requirement about your source code ......................................................................................... 2 Format of the Project Final Report .............................................................................................. 2 Content of the Project Final Report ............................................................................................. 2 Abstract (within 300 words in a separate page) ........................................................................................... 2 Section 1 Feature Engineering ................................................................................................................... 2 Section 2 Models and Performance (within four pages) ............................................................................ 2 Section 3 Contribution and Justification ................................................................................................... 3 Section 4 Reference (within one page) ........................................................................................................ 4 Assessment Rubrics (40 marks) ................................................................................................... 4  Deliverable List \n"," Note:  1. Both the source code links (or file names) and dataset links should be listed in a pdf file with clear, precise, simple and short explanation about their purpose and content. This pdf file should be also uploaded into Canvas by 11:59 PM Nov. 13. 2. Late submission of source code and datasets will result in the absence of the corresponding evaluation. Deliverable Format Due Time Project Final Presentation .pptx uploaded to Canvas (we will upload your files into a laptop (with windows OS) in advance for your presentation purpose.) 11:59 PM Nov. 14 (Thur.)  Project Final Report .pdf or .docx uploaded to Canvas 11:59 PM Nov. 17 (Sun)  Project source code .ipynb files uploaded to Canvas or accessible via a public link 11:59 PM Nov. 13 (Wed)  Datasets .csv files including both raw datasets and intermediary datasets if applicable). Please provide the links via which the datasets can be accessed. Do not upload your data to Canvas. 11:59 PM Nov. 13 (Wed)  Project group peer evaluation form .doc or docx file uploaded to Canvas 11:59 PM Nov. 17 (Sun)  #Document \n","text: ©QIUHONG WANG 2024 2 Requirement about your source code 1. Source code file name should be as self-explainable as possible. For example, step1_data_preprocess.ipynb, step2_classification_models.ipynb, step3_rnn_models.ipynb. 2. Source code files should be able to directly run via Google colab, including loading relevant raw datasets or intermediary datasets. If it is indeed not feasible, please provide a readme file to specify the running environment.  3. Source code files should be able to reproduce your reported model performance (reasonable deviation is acceptable). To serve this purpose, please highlight in your source code the scripts that are used to generate the reported performance. Please attach or keep such performance report in the source code for verification purpose.  4. Please provide necessary markdown to explain the purpose, function or approach of the important sections in your source code, in particular those that are relevant with your elaboration in the report. Format of the Project Final Report • Cover page: Project Topic, Group No., List of group members • Line spacing: 1.5 lines • Font: Arial, Calibri, or Times New Roman; Size 11 • Margins: Normal; Columns: One Content of the Project Final Report Your project report should include the following sections.  Abstract (within 300 words in a separate page)  • Summarize the business issue and the corresponding machine learning problem that have been addressed in this project. (please make sure the business issue and the machine learning problem are consistent with each other. Inaccuracy or mismatching between them will be penalized.) • Acknowledge the datasets that have been utilized and the established models (or source code) that have been adapted into your project. (details about the sources should be provided in reference, not here ). • Summarize in bullet points any achievement or noteworthy highlights directly attributed to your group’s efforts.   Section 1 Feature Engineering Instead of repeating the data description provided in your project interim report, please focus on updating the changes or the incremental data or features included in your projects, if necessary. You can follow the same format as in your project interim report by using tables.  Section 2 Models and Performance (within four pages) • Use diagrams to illustrate the models and/or model architecture that have been used in your project. Precise and succinct explanation should be provided if necessary. \n","    \n"]}],"source":["print('Example of the retrieval output fed to the LLM: \\n')\n","print(just_unstructured_retriever(demo_question))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qu4v1y2rDO_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c46b6ff-448a-4ee7-d22d-90d383208ee9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unstructured retrieval model response: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["To ensure satisfactory contribution to the course project, it is important to\n","use valuable and high-quality datasets, be creative in feature engineering,\n","design or adapt new ML methods/architecture, and provide objective\n","justifications for self-evaluation. Additionally, source code should be well-\n","organized, reproducible, and include necessary markdown explanations. Following\n","the project deliverable guidelines and assessment rubrics is crucial for meeting\n","the project requirements.\n"]}],"source":["print('Unstructured retrieval model response: \\n')\n","response = unstructured_chain.invoke({\"question\": demo_question})\n","wrapped_response = textwrap.fill(response, width=80)\n","print(wrapped_response)"]},{"cell_type":"markdown","metadata":{"id":"1PNclEiCrDO_"},"source":["#####<h3>Demonstration 2: Structured Retrevial</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGo-v4HfrDO_"},"outputs":[],"source":["# Retrieval\n","def just_structured_retriever(question: str):\n","    print(f\"Search query: {question}\")\n","    structured_data = structured_retriever(question)\n","   # unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n","    final_data = f\"\"\"Structured data:\n","{structured_data}\n","    \"\"\"\n","    return final_data\n","\n","# Prompt Augumentation: it instructs the model to answer a question using only the context provided.\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","Use natural language and be concise.\n","Answer:\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n","structured_chain = (\n","    RunnableParallel(\n","        {\n","            \"context\": _search_query | just_structured_retriever,\n","            \"question\": RunnablePassthrough(),\n","        }\n","    )\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbQWLYM_rDO_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bb02e7b-8510-42bd-b74a-5229778dad78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the retrieval output fed to the LLM: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n","Structured data:\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","    \n"]}],"source":["print('Example of the retrieval output fed to the LLM: \\n')\n","print(just_structured_retriever(demo_question))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDgD7pYBrDO_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb36ac4c-d3be-4f6a-bbbd-6a561d9ef787"},"outputs":[{"output_type":"stream","name":"stdout","text":["Structured retrieval model response: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n","To ensure satisfactory contribution in completing the course project, it is best\n","practice to upload project source code, final presentation, and final report to\n","Canvas, make them accessible via public links, focus on datasets and feature\n","engineering, utilize ML methods and bias identification, enhance insights and\n","creativity, explain prediction results, identify bias and gaps, impact business\n","decision making, utilize objective evidences for self-evaluation justification,\n","ensure source code quality, understand data, present with clarity, logic, and\n","succinctness, belong to a group, include presentation and Q&A, and avoid using\n","scripts.\n"]}],"source":["print('Structured retrieval model response: \\n')\n","response = structured_chain.invoke({\"question\": demo_question})\n","wrapped_response = textwrap.fill(response, width=80)\n","print(wrapped_response)"]},{"cell_type":"markdown","metadata":{"id":"Jb-7MIz7rDPA"},"source":["As we can see from the llm's response, the model benefits from the structured data returned and gives out more detailed and structured elaborations. For some responses, the model managed to state that students are graded based on tutorial assignments, lab sessions and are tested on Datacamp Assignments.\n","\n","However, we can also see that the response gets the course instructor wrong due to the lack of context. This is where combining the structured and unstructured retiever greatly improves the response."]},{"cell_type":"markdown","metadata":{"id":"OUM260nmrDPA"},"source":["####<h3>Demonstration 3: Hybrid Retrevial</h3>"]},{"cell_type":"markdown","metadata":{"id":"2QcQfUm9rDPA"},"source":["Finally, we can go ahead and test our hybrid RAG implementation.\n","We will use **retriever** defined in Step 3b.2.3, that combines the unstructured and structured (Hybrid) retriever"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DstwJ9JfrDPA"},"outputs":[],"source":["# Retrieval\n","# refer to retriever defined in Step 3b.2.3\n","\n","\n","# Prompt Augumentation: it instructs the model to answer a question using only the context provided.\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","Use natural language and be concise.\n","Answer:\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n","final_chain = (\n","    RunnableParallel(\n","        {\n","            \"context\": _search_query | retriever,\n","            \"question\": RunnablePassthrough(),\n","        }\n","    )\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F52E8Z4VrDPA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41328117-e6c5-44f4-d44c-9f791fb75058"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example of the retrieval output fed to the LLM: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["Structured data:\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Project Final Presentation - UPLOADED_TO -> Canvas\n","Project Final Report - UPLOADED_TO -> Canvas\n","Project Source Code - UPLOADED_TO -> Canvas\n","Project Source Code - ACCESSIBLE_VIA -> Public Link\n","Datasets - UPLOADED_TO -> Canvas\n","Datasets - ACCESSIBLE_VIA -> Public Link\n","Source_Code_File - CONTAINS -> Model_Performance\n","Source_Code_File - CONTAINS -> Markdown_Explanation\n","Source_Code_File - RUNNABLE_IN -> Google_Colab\n","Model_Performance - INCLUDES -> Training_Set\n","Model_Performance - INCLUDES -> Testing_Set\n","Model_Performance - INCLUDES -> Source_Code\n","Project_Report - CONTAINS -> Abstract\n","Project_Report - CONTAINS -> Feature_Engineering\n","Project_Report - CONTAINS -> Models_Performance\n","Contribution_Justification - FOCUS -> Datasets\n","Contribution_Justification - FOCUS -> Feature_Engineering\n","Contribution_Justification - FOCUS -> Aspects_Contribution\n","Contribution_Justification - FOCUS -> Ml_Methods_Architecture\n","Ensemble Learning - INTEGRATE -> Ml Models\n","Ml Models - DESIGN -> Innovative Architecture\n","Ml Models - DESIGN -> Pipeline\n","Ml Models - ADAPT -> Ml Methods\n","Ml Methods - PROPOSED_METHOD -> Research Papers\n","Ml Methods - UTILIZES -> Bias Identification\n","Creativity - ENHANCE -> Insights\n","Prediction Results - EXPLAIN -> Creativity\n","Prediction Results - EXPLAIN -> Insights\n","Prediction Results - IDENTIFY -> Bias\n","Bias - IDENTIFY -> Gap\n","Gap - IDENTIFY -> Confusion\n","Confusion - IMPACT -> Business Decision Making\n","Data Analytics Methods - UTILIZES -> Ml Methods\n","Data Analytics Methods - UTILIZES -> Econometrics\n","Data Analytics Methods - UTILIZES -> Network Analysis\n","Bias Identification - UTILIZES -> Ml Output Evaluation\n","Ml Output Evaluation - UTILIZES -> Business Decision Making\n","Self-Evaluation Justification - UTILIZES -> Objective Evidences\n","Source Code Quality - UTILIZES -> Data Understanding\n","Data Understanding - UTILIZES -> Presentation\n","Presentation - HAS -> Clarity\n","Presentation - HAS -> Logic\n","Presentation - HAS -> Succinctness\n","Group - BELONGS_TO -> Member\n","Group - INCLUDES -> Presentation\n","Group - INCLUDES -> Q&A\n","Group - NOT_ALLOWED -> Script\n","Unstructured data:\n","\n","text: ©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2#Document \n","text: ©QIUHONG WANG 2024 4 Level 2-3: Use data analyDcs methods (econometrics, network analysis, etc. what you have learned from the other courses) and addiDonal ML methods to clarify, disDnguish, idenDfy the bias of, or evaluate your ML output towards eﬀecDve business decision making.   Any aspects that are dis)nct from the above…  ObjecDve evidences are needed to suﬃciently jusDfy your self-evaluaDon.  • In your report, provide justification for your self-evaluation with necessary details and objective evidences. Without objective justification, the claimed contribution cannot be supported. For each aspect of your contribution, your justification should include but not limit itself to the following. o Necessary information for people to understand what you have actually done. o How does your approach differ from existing methods? You may cite established projects, papers, or the provided example source code and highlight the deviations from them. When using existing projects or papers as benchmarks, ensure they are widely recognized and not considered as noise in the field. o Any other objective and convincing evidence that can be reproduced from your models. o You should not claim a contribution that has not been ultimately used in generating your main results and the source code of which is not provided in your deliverables. Section 4 Reference (within one page) Reference must be comprehensive with correct information and valid links if applicable.  Assessment Rubrics (40 marks) § Ontime submission of all the deliverables following required format (1 mark) § Completion of the project that can sufficiently address your proposed machine learning problem by using appropriate approaches with reasonable performance; and provision of required Abstract, Sections 1-2 and 4 in your report systematically with clarity and simplicity (14 marks) § Source code quality: your source code should be well organized with modularity and following a systematic pipeline, with clear and friendly-readable markdown. (5 marks) § Understanding your data and source code to the extent that any member can answer questions related to your data or source code during your presentation (2 mark) § Presentation (4 marks) Your presentation should communicate the important information relevant to Section 1, 2, and 3.   § Contribution from the four aspects defined in the table above (14 marks)  Presentation evaluation: § Basic requirement (1 mark) o 10 minutes per group excluding Q&A; o Every group#Document \n","text: ©QIUHONG WANG 2024 1 BT4222 Project Deliverable Guideline  August 2024 Table of Contents Deliverable List ........................................................................................................................... 1 Requirement about your source code ......................................................................................... 2 Format of the Project Final Report .............................................................................................. 2 Content of the Project Final Report ............................................................................................. 2 Abstract (within 300 words in a separate page) ........................................................................................... 2 Section 1 Feature Engineering ................................................................................................................... 2 Section 2 Models and Performance (within four pages) ............................................................................ 2 Section 3 Contribution and Justification ................................................................................................... 3 Section 4 Reference (within one page) ........................................................................................................ 4 Assessment Rubrics (40 marks) ................................................................................................... 4  Deliverable List \n"," Note:  1. Both the source code links (or file names) and dataset links should be listed in a pdf file with clear, precise, simple and short explanation about their purpose and content. This pdf file should be also uploaded into Canvas by 11:59 PM Nov. 13. 2. Late submission of source code and datasets will result in the absence of the corresponding evaluation. Deliverable Format Due Time Project Final Presentation .pptx uploaded to Canvas (we will upload your files into a laptop (with windows OS) in advance for your presentation purpose.) 11:59 PM Nov. 14 (Thur.)  Project Final Report .pdf or .docx uploaded to Canvas 11:59 PM Nov. 17 (Sun)  Project source code .ipynb files uploaded to Canvas or accessible via a public link 11:59 PM Nov. 13 (Wed)  Datasets .csv files including both raw datasets and intermediary datasets if applicable). Please provide the links via which the datasets can be accessed. Do not upload your data to Canvas. 11:59 PM Nov. 13 (Wed)  Project group peer evaluation form .doc or docx file uploaded to Canvas 11:59 PM Nov. 17 (Sun)  #Document \n","text: ©QIUHONG WANG 2024 2 Requirement about your source code 1. Source code file name should be as self-explainable as possible. For example, step1_data_preprocess.ipynb, step2_classification_models.ipynb, step3_rnn_models.ipynb. 2. Source code files should be able to directly run via Google colab, including loading relevant raw datasets or intermediary datasets. If it is indeed not feasible, please provide a readme file to specify the running environment.  3. Source code files should be able to reproduce your reported model performance (reasonable deviation is acceptable). To serve this purpose, please highlight in your source code the scripts that are used to generate the reported performance. Please attach or keep such performance report in the source code for verification purpose.  4. Please provide necessary markdown to explain the purpose, function or approach of the important sections in your source code, in particular those that are relevant with your elaboration in the report. Format of the Project Final Report • Cover page: Project Topic, Group No., List of group members • Line spacing: 1.5 lines • Font: Arial, Calibri, or Times New Roman; Size 11 • Margins: Normal; Columns: One Content of the Project Final Report Your project report should include the following sections.  Abstract (within 300 words in a separate page)  • Summarize the business issue and the corresponding machine learning problem that have been addressed in this project. (please make sure the business issue and the machine learning problem are consistent with each other. Inaccuracy or mismatching between them will be penalized.) • Acknowledge the datasets that have been utilized and the established models (or source code) that have been adapted into your project. (details about the sources should be provided in reference, not here ). • Summarize in bullet points any achievement or noteworthy highlights directly attributed to your group’s efforts.   Section 1 Feature Engineering Instead of repeating the data description provided in your project interim report, please focus on updating the changes or the incremental data or features included in your projects, if necessary. You can follow the same format as in your project interim report by using tables.  Section 2 Models and Performance (within four pages) • Use diagrams to illustrate the models and/or model architecture that have been used in your project. Precise and succinct explanation should be provided if necessary. \n","    \n"]}],"source":["print('Example of the retrieval output fed to the LLM: \\n')\n","print(retriever(demo_question))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvP0fmJurDPA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b61793b-f032-4684-c7ab-bcb50344cde7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hybrid retrieval model response: \n","\n","Search query: What are the best practices for completing the course project to ensure satisfactory contribution?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"stream","name":"stdout","text":["To ensure satisfactory contribution for the course project, it is important to\n","follow these best practices: 1. Use valuable and high-quality datasets,\n","including integrating existing datasets or collecting new data from multiple\n","sources. 2. Be creative in feature engineering by generating new features based\n","on relevant theories or domain knowledge. 3. Design or adapt new ML\n","methods/architecture with a balance of resources and costs. 4. Utilize data\n","analytics methods and additional ML methods to clarify, distinguish, identify\n","bias, or evaluate ML output for effective business decision-making. 5. Provide\n","objective evidence to justify self-evaluation, including necessary details and\n","reproducible outcomes from your models.\n"]}],"source":["print('Hybrid retrieval model response: \\n')\n","response = final_chain.invoke({\"question\": demo_question})\n","wrapped_response = textwrap.fill(response, width=80)\n","print(wrapped_response)"]},{"cell_type":"markdown","metadata":{"id":"4_xUet-frDPA"},"source":["Now that we have completed the demonstrations, we will show how to incorporate chat history for follow up questions to the LLM. We will use the response from demonstration 3 to continue the conversation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0HY1pwKrDPA","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"2416b2e3-3143-4dfa-cd36-8c25da03f120"},"outputs":[{"output_type":"stream","name":"stdout","text":["Search query: What was asked in the previous question?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"]},{"output_type":"execute_result","data":{"text/plain":["'You asked about the best practices for completing the course project to ensure satisfactory contribution.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":114}],"source":["question_step3b3_2 = \"What did i ask in the previous question?\"\n","previous_qn = demo_question\n","previous_res = wrapped_response\n","final_chain.invoke(\n","    {\n","        \"question\": question_step3b3_2,\n","        \"chat_history\": [(previous_qn, previous_res)],\n","    }\n",")"]},{"cell_type":"markdown","source":["# **Question** (4 marks)\n","\n","**Part 1**. Identify a domain or scenario where the LLM is unlikely trained on the documents from this domain or scenario and at the same time the application of LLM+RAG will be valuable. (1 mark)\n","\n","For example, LLM is unlikely trained on the BT4222 deliverable guideline document. This is why it was used in this example.  \n","\n","Adapt this exampe source code into a document from your proposed domain and demonstrate how LLM+RAG could help you better understand specific queries related to this domain.\n","\n","**Part 2**. Please list your query and the best responses from LLM (1 mark)\n","\n","**Part 3**. Please report which part of the source code has been revised in order to fit for the specific domain (if applicable) (2 mark)\n","\n","*Hint*:\n","- By different content, the entities to be detected from your query or input can be differernt.\n","- PyPDFLoader is weak in extracting content from table.\n","\n","To be rewarded the 3 marks from Part 2 and Part 3, please maintain all output cells generated by the source code above in your submitted notebook file to provide evidence of your results."],"metadata":{"id":"HXWy2cVg86XI"}},{"cell_type":"markdown","source":["**Your answer for part 1**:"],"metadata":{"id":"fJBGB0Cp-VUf"}},{"cell_type":"markdown","source":["**Your answer for part 2**:"],"metadata":{"id":"1weUbXdP-alb"}},{"cell_type":"markdown","source":["**Your answer for part 3**:"],"metadata":{"id":"1gZB3iF3fG-w"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"2cb0875157294be6a9d673a82ed6816e":{"model_module":"yfiles-jupyter-graphs","model_name":"GraphModel","model_module_version":"^1.8.1","state":{"_context_pane_mapping":[{"id":"Neighborhood","title":"Neighborhood"},{"id":"Data","title":"Data"},{"id":"Search","title":"Search"},{"id":"About","title":"About"}],"_data_importer":"neo4j","_directed":true,"_dom_classes":[],"_edges":[{"id":1159676904047902700,"start":114,"end":118,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#9C27B0","thickness_factor":1,"directed":true},{"id":6924380084593754000,"start":118,"end":120,"properties":{"label":"UPLOADED_TO"},"label":"UPLOADED_TO","color":"#2196F3","thickness_factor":1,"directed":true},{"id":6919877584478011000,"start":118,"end":121,"properties":{"label":"ACCESSIBLE_VIA"},"label":"ACCESSIBLE_VIA","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1168684103302644000,"start":131,"end":118,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#9C27B0","thickness_factor":1,"directed":true},{"id":1155272260467032300,"start":135,"end":118,"properties":{"label":"FOCUS"},"label":"FOCUS","color":"#F44336","thickness_factor":1,"directed":true},{"id":1152921504606847200,"start":153,"end":154,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#9C27B0","thickness_factor":1,"directed":true},{"id":1175439502743699700,"start":153,"end":162,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#9C27B0","thickness_factor":1,"directed":true},{"id":1157530657350484200,"start":154,"end":144,"properties":{"label":"UTILIZES"},"label":"UTILIZES","color":"#607D8B","thickness_factor":1,"directed":true},{"id":1153027057723113700,"start":154,"end":155,"properties":{"label":"UTILIZES"},"label":"UTILIZES","color":"#607D8B","thickness_factor":1,"directed":true},{"id":1155278857536799000,"start":154,"end":156,"properties":{"label":"UTILIZES"},"label":"UTILIZES","color":"#607D8B","thickness_factor":1,"directed":true},{"id":1153027057723113700,"start":161,"end":162,"properties":{"label":"UTILIZES"},"label":"UTILIZES","color":"#607D8B","thickness_factor":1,"directed":true},{"id":1153027057723113700,"start":162,"end":163,"properties":{"label":"UTILIZES"},"label":"UTILIZES","color":"#607D8B","thickness_factor":1,"directed":true}],"_graph_layout":{},"_highlight":[],"_license":{},"_model_module":"yfiles-jupyter-graphs","_model_module_version":"^1.8.1","_model_name":"GraphModel","_neighborhood":{},"_nodes":[{"id":114,"properties":{"id":"f371ca77fa892a55c1c59fe174b7a669","text":"©QIUHONG WANG 2024 1 BT4222 Project Deliverable Guideline  August 2024 Table of Contents Deliverable List ........................................................................................................................... 1 Requirement about your source code ......................................................................................... 2 Format of the Project Final Report .............................................................................................. 2 Content of the Project Final Report ............................................................................................. 2 Abstract (within 300 words in a separate page) ........................................................................................... 2 Section 1 Feature Engineering ................................................................................................................... 2 Section 2 Models and Performance (within four pages) ............................................................................ 2 Section 3 Contribution and Justification ................................................................................................... 3 Section 4 Reference (within one page) ........................................................................................................ 4 Assessment Rubrics (40 marks) ................................................................................................... 4  Deliverable List \n Note:  1. Both the source code links (or file names) and dataset links should be listed in a pdf file with clear, precise, simple and short explanation about their purpose and content. This pdf file should be also uploaded into Canvas by 11:59 PM Nov. 13. 2. Late submission of source code and datasets will result in the absence of the corresponding evaluation. Deliverable Format Due Time Project Final Presentation .pptx uploaded to Canvas (we will upload your files into a laptop (with windows OS) in advance for your presentation purpose.) 11:59 PM Nov. 14 (Thur.)  Project Final Report .pdf or .docx uploaded to Canvas 11:59 PM Nov. 17 (Sun)  Project source code .ipynb files uploaded to Canvas or accessible via a public link 11:59 PM Nov. 13 (Wed)  Datasets .csv files including both raw datasets and intermediary datasets if applicable). Please provide the links via which the datasets can be accessed. Do not upload your data to Canvas. 11:59 PM Nov. 13 (Wed)  Project group peer evaluation form .doc or docx file uploaded to Canvas 11:59 PM Nov. 17 (Sun)  ","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":0,"label":"Document"},"color":"#2196F3","styles":{},"label":"f371ca77fa892a55c1c59fe174b7a669","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":118,"properties":{"id":"Datasets","label":"__Entity__:Deliverable:Aspect"},"color":"#4CAF50","styles":{},"label":"Datasets","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":120,"properties":{"id":"Canvas","label":"__Entity__"},"color":"#F44336","styles":{},"label":"Canvas","scale_factor":1,"type":"#F44336","size":[55,55],"position":[0,0]},{"id":121,"properties":{"id":"Public Link","label":"__Entity__"},"color":"#F44336","styles":{},"label":"Public Link","scale_factor":1,"type":"#F44336","size":[55,55],"position":[0,0]},{"id":131,"properties":{"id":"34cee84a34dc6003c5eb892236abc2c2","text":"©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":2,"label":"Document"},"color":"#2196F3","styles":{},"label":"34cee84a34dc6003c5eb892236abc2c2","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":135,"properties":{"id":"Contribution_Justification","label":"__Entity__:Section"},"color":"#607D8B","styles":{},"label":"Contribution_Justification","scale_factor":1,"type":"#607D8B","size":[55,55],"position":[0,0]},{"id":153,"properties":{"id":"2543e9636d1864a6ecb2a2459eb0f39b","text":"©QIUHONG WANG 2024 4 Level 2-3: Use data analyDcs methods (econometrics, network analysis, etc. what you have learned from the other courses) and addiDonal ML methods to clarify, disDnguish, idenDfy the bias of, or evaluate your ML output towards eﬀecDve business decision making.   Any aspects that are dis)nct from the above…  ObjecDve evidences are needed to suﬃciently jusDfy your self-evaluaDon.  • In your report, provide justification for your self-evaluation with necessary details and objective evidences. Without objective justification, the claimed contribution cannot be supported. For each aspect of your contribution, your justification should include but not limit itself to the following. o Necessary information for people to understand what you have actually done. o How does your approach differ from existing methods? You may cite established projects, papers, or the provided example source code and highlight the deviations from them. When using existing projects or papers as benchmarks, ensure they are widely recognized and not considered as noise in the field. o Any other objective and convincing evidence that can be reproduced from your models. o You should not claim a contribution that has not been ultimately used in generating your main results and the source code of which is not provided in your deliverables. Section 4 Reference (within one page) Reference must be comprehensive with correct information and valid links if applicable.  Assessment Rubrics (40 marks) § Ontime submission of all the deliverables following required format (1 mark) § Completion of the project that can sufficiently address your proposed machine learning problem by using appropriate approaches with reasonable performance; and provision of required Abstract, Sections 1-2 and 4 in your report systematically with clarity and simplicity (14 marks) § Source code quality: your source code should be well organized with modularity and following a systematic pipeline, with clear and friendly-readable markdown. (5 marks) § Understanding your data and source code to the extent that any member can answer questions related to your data or source code during your presentation (2 mark) § Presentation (4 marks) Your presentation should communicate the important information relevant to Section 1, 2, and 3.   § Contribution from the four aspects defined in the table above (14 marks)  Presentation evaluation: § Basic requirement (1 mark) o 10 minutes per group excluding Q&A; o Every group","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":3,"label":"Document"},"color":"#2196F3","styles":{},"label":"2543e9636d1864a6ecb2a2459eb0f39b","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":154,"properties":{"id":"Data Analytics Methods","label":"__Entity__:Method"},"color":"#673AB7","styles":{},"label":"Data Analytics Methods","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":162,"properties":{"id":"Data Understanding","label":"__Entity__:Method"},"color":"#673AB7","styles":{},"label":"Data Understanding","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":144,"properties":{"id":"Ml Methods","label":"__Entity__:Method:Machine learning"},"color":"#CDDC39","styles":{},"label":"Ml Methods","scale_factor":1,"type":"#CDDC39","size":[55,55],"position":[0,0]},{"id":155,"properties":{"id":"Econometrics","label":"__Entity__:Method"},"color":"#673AB7","styles":{},"label":"Econometrics","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":156,"properties":{"id":"Network Analysis","label":"__Entity__:Method"},"color":"#673AB7","styles":{},"label":"Network Analysis","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":161,"properties":{"id":"Source Code Quality","label":"__Entity__:Method"},"color":"#673AB7","styles":{},"label":"Source Code Quality","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":163,"properties":{"id":"Presentation","label":"__Entity__:Method:Presentation:Quality"},"color":"#9E9E9E","styles":{},"label":"Presentation","scale_factor":1,"type":"#9E9E9E","size":[55,55],"position":[0,0]}],"_overview":{"enabled":null,"overview_set":false},"_selected_graph":[[],[]],"_sidebar":{"enabled":false,"start_with":null},"_view_count":null,"_view_module":"yfiles-jupyter-graphs","_view_module_version":"^1.8.1","_view_name":"GraphView","layout":"IPY_MODEL_3809b48b07e34490aaf63237df4cd32a","tabbable":null,"tooltip":null}},"3809b48b07e34490aaf63237df4cd32a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"640px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"6915e97f75024830bac4fd12c8445957":{"model_module":"yfiles-jupyter-graphs","model_name":"GraphModel","model_module_version":"^1.8.1","state":{"_context_pane_mapping":[{"id":"Neighborhood","title":"Neighborhood"},{"id":"Data","title":"Data"},{"id":"Search","title":"Search"},{"id":"About","title":"About"}],"_data_importer":"neo4j","_directed":true,"_dom_classes":[],"_edges":[{"id":1152921504606847000,"start":114,"end":115,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1155173304420532200,"start":114,"end":116,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1157425104234217500,"start":114,"end":117,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1159676904047902700,"start":114,"end":118,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1161928703861588000,"start":114,"end":119,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1152921504606847000,"start":122,"end":123,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1155173304420532200,"start":122,"end":124,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1157425104234217500,"start":122,"end":125,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1159676904047902700,"start":122,"end":126,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1161928703861588000,"start":122,"end":127,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1164180503675273200,"start":122,"end":128,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1166432303488958500,"start":122,"end":129,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1168684103302643700,"start":122,"end":130,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1168684103302644000,"start":131,"end":118,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1152921504606847200,"start":131,"end":125,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1170935903116329200,"start":131,"end":129,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1155173304420532500,"start":131,"end":132,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1157425104234217700,"start":131,"end":133,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1159676904047903000,"start":131,"end":134,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1161928703861588200,"start":131,"end":135,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1164180503675273500,"start":131,"end":136,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1166432303488958700,"start":131,"end":137,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1173187702930014500,"start":131,"end":138,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1152921504606847200,"start":139,"end":140,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true},{"id":1155173304420532500,"start":139,"end":141,"properties":{"label":"MENTIONS"},"label":"MENTIONS","color":"#4CAF50","thickness_factor":1,"directed":true}],"_graph_layout":{},"_highlight":[],"_license":{},"_model_module":"yfiles-jupyter-graphs","_model_module_version":"^1.8.1","_model_name":"GraphModel","_neighborhood":{},"_nodes":[{"id":114,"properties":{"id":"f371ca77fa892a55c1c59fe174b7a669","text":"©QIUHONG WANG 2024 1 BT4222 Project Deliverable Guideline  August 2024 Table of Contents Deliverable List ........................................................................................................................... 1 Requirement about your source code ......................................................................................... 2 Format of the Project Final Report .............................................................................................. 2 Content of the Project Final Report ............................................................................................. 2 Abstract (within 300 words in a separate page) ........................................................................................... 2 Section 1 Feature Engineering ................................................................................................................... 2 Section 2 Models and Performance (within four pages) ............................................................................ 2 Section 3 Contribution and Justification ................................................................................................... 3 Section 4 Reference (within one page) ........................................................................................................ 4 Assessment Rubrics (40 marks) ................................................................................................... 4  Deliverable List \n Note:  1. Both the source code links (or file names) and dataset links should be listed in a pdf file with clear, precise, simple and short explanation about their purpose and content. This pdf file should be also uploaded into Canvas by 11:59 PM Nov. 13. 2. Late submission of source code and datasets will result in the absence of the corresponding evaluation. Deliverable Format Due Time Project Final Presentation .pptx uploaded to Canvas (we will upload your files into a laptop (with windows OS) in advance for your presentation purpose.) 11:59 PM Nov. 14 (Thur.)  Project Final Report .pdf or .docx uploaded to Canvas 11:59 PM Nov. 17 (Sun)  Project source code .ipynb files uploaded to Canvas or accessible via a public link 11:59 PM Nov. 13 (Wed)  Datasets .csv files including both raw datasets and intermediary datasets if applicable). Please provide the links via which the datasets can be accessed. Do not upload your data to Canvas. 11:59 PM Nov. 13 (Wed)  Project group peer evaluation form .doc or docx file uploaded to Canvas 11:59 PM Nov. 17 (Sun)  ","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":0,"label":"Document"},"color":"#2196F3","styles":{},"label":"f371ca77fa892a55c1c59fe174b7a669","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":115,"properties":{"id":"Project Final Presentation","label":"__Entity__:Deliverable"},"color":"#4CAF50","styles":{},"label":"Project Final Presentation","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":116,"properties":{"id":"Project Final Report","label":"__Entity__:Deliverable"},"color":"#4CAF50","styles":{},"label":"Project Final Report","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":117,"properties":{"id":"Project Source Code","label":"__Entity__:Deliverable"},"color":"#4CAF50","styles":{},"label":"Project Source Code","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":118,"properties":{"id":"Datasets","label":"__Entity__:Deliverable:Aspect"},"color":"#F44336","styles":{},"label":"Datasets","scale_factor":1,"type":"#F44336","size":[55,55],"position":[0,0]},{"id":119,"properties":{"id":"Project Group Peer Evaluation Form","label":"__Entity__:Deliverable"},"color":"#4CAF50","styles":{},"label":"Project Group Peer Evaluation Form","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":122,"properties":{"id":"53706689182b164b4d5c5f14fe0ce763","text":"©QIUHONG WANG 2024 2 Requirement about your source code 1. Source code file name should be as self-explainable as possible. For example, step1_data_preprocess.ipynb, step2_classification_models.ipynb, step3_rnn_models.ipynb. 2. Source code files should be able to directly run via Google colab, including loading relevant raw datasets or intermediary datasets. If it is indeed not feasible, please provide a readme file to specify the running environment.  3. Source code files should be able to reproduce your reported model performance (reasonable deviation is acceptable). To serve this purpose, please highlight in your source code the scripts that are used to generate the reported performance. Please attach or keep such performance report in the source code for verification purpose.  4. Please provide necessary markdown to explain the purpose, function or approach of the important sections in your source code, in particular those that are relevant with your elaboration in the report. Format of the Project Final Report • Cover page: Project Topic, Group No., List of group members • Line spacing: 1.5 lines • Font: Arial, Calibri, or Times New Roman; Size 11 • Margins: Normal; Columns: One Content of the Project Final Report Your project report should include the following sections.  Abstract (within 300 words in a separate page)  • Summarize the business issue and the corresponding machine learning problem that have been addressed in this project. (please make sure the business issue and the machine learning problem are consistent with each other. Inaccuracy or mismatching between them will be penalized.) • Acknowledge the datasets that have been utilized and the established models (or source code) that have been adapted into your project. (details about the sources should be provided in reference, not here ). • Summarize in bullet points any achievement or noteworthy highlights directly attributed to your group’s efforts.   Section 1 Feature Engineering Instead of repeating the data description provided in your project interim report, please focus on updating the changes or the incremental data or features included in your projects, if necessary. You can follow the same format as in your project interim report by using tables.  Section 2 Models and Performance (within four pages) • Use diagrams to illustrate the models and/or model architecture that have been used in your project. Precise and succinct explanation should be provided if necessary. ","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":1,"label":"Document"},"color":"#2196F3","styles":{},"label":"53706689182b164b4d5c5f14fe0ce763","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":123,"properties":{"id":"Source_Code_File","label":"__Entity__:File"},"color":"#607D8B","styles":{},"label":"Source_Code_File","scale_factor":1,"type":"#607D8B","size":[55,55],"position":[0,0]},{"id":124,"properties":{"id":"Google_Colab","label":"__Entity__:Environment"},"color":"#673AB7","styles":{},"label":"Google_Colab","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":125,"properties":{"id":"Model_Performance","label":"__Entity__:Performance_report:Section"},"color":"#CDDC39","styles":{},"label":"Model_Performance","scale_factor":1,"type":"#CDDC39","size":[55,55],"position":[0,0]},{"id":126,"properties":{"id":"Markdown_Explanation","label":"__Entity__:Markdown"},"color":"#9E9E9E","styles":{},"label":"Markdown_Explanation","scale_factor":1,"type":"#9E9E9E","size":[55,55],"position":[0,0]},{"id":127,"properties":{"id":"Project_Report","label":"__Entity__:Report"},"color":"#9C27B0","styles":{},"label":"Project_Report","scale_factor":1,"type":"#9C27B0","size":[55,55],"position":[0,0]},{"id":128,"properties":{"id":"Abstract","label":"__Entity__:Abstract"},"color":"#2196F3","styles":{},"label":"Abstract","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":129,"properties":{"id":"Feature_Engineering","label":"__Entity__:Feature_engineering:Aspect"},"color":"#4CAF50","styles":{},"label":"Feature_Engineering","scale_factor":1,"type":"#4CAF50","size":[55,55],"position":[0,0]},{"id":130,"properties":{"id":"Models_Performance","label":"__Entity__:Models_performance"},"color":"#F44336","styles":{},"label":"Models_Performance","scale_factor":1,"type":"#F44336","size":[55,55],"position":[0,0]},{"id":131,"properties":{"id":"34cee84a34dc6003c5eb892236abc2c2","text":"©QIUHONG WANG 2024 3 • Use table and/or figures to report the model performance on both the training set and the testing sets (all these outcome can be reproduced by running your source code). Precise and succinct explanation should be provided if necessary.  This section is a general requirement for any machine learning related project. Let us keep it simple and straightforward with only necessary information.  Section 3 Contribution and Justification  Among the four aspects identified below or your unique aspects that are not listed here, please provide your justification within 1-3 pages for each aspect. This excludes the self-evaluation table.  • Complete the following table and assess your own contribution taken into account both the extent of efforts and the effectiveness of the outcomes.  Regarding the four aspects of the contribution, you are not required to cover all of them in your project. It should be your own decision depending on your interest, and an optimization of your studying efforts and outcome. For instance, you may focus on only one aspect that you are most interested.  Our evaluation will assess your efforts and effectiveness in each aspect, considering the competitiveness among groups. Please note that “Low” level contribution is not a negative term here.   Contribu)on   Use valuable and high-quality datasets, including integraDng exisDng datasets, crawling or retrieving data, etc.     Level 1: Use exisDng datasets acquired directly from external resources  Level 2 - 3: collecDng new data or integrate datasets from mulDple sources considering the richness of the informaDon, the amount of data points, the creaDvity in ﬁnding surprisingly useful resources or the intelligence in integraDng datasets for new insights  Crea)vity in feature engineering   Level 1: generate new features based on descripDve staDsDcs (e.g., mean, variance, etc.) Level 2-3: generate new features by applying relevant theories, domain knowledge, representaDve learning, social network analysis, or other machine learning or econometrics methods. AdopDng methods proposed by high-quality research papers is also encouraged.   Design or adapta)on of new ML methods/architecture or the integra)on of exis)ng methods with a balance of resource and cost  Level 1: ensemble learning by straighRorwardly integraDng mulDple ML models Level 2","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":2,"label":"Document"},"color":"#2196F3","styles":{},"label":"34cee84a34dc6003c5eb892236abc2c2","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":132,"properties":{"id":"Training_Set","label":"__Entity__:Dataset"},"color":"#607D8B","styles":{},"label":"Training_Set","scale_factor":1,"type":"#607D8B","size":[55,55],"position":[0,0]},{"id":133,"properties":{"id":"Testing_Set","label":"__Entity__:Dataset"},"color":"#607D8B","styles":{},"label":"Testing_Set","scale_factor":1,"type":"#607D8B","size":[55,55],"position":[0,0]},{"id":134,"properties":{"id":"Source_Code","label":"__Entity__:Code"},"color":"#673AB7","styles":{},"label":"Source_Code","scale_factor":1,"type":"#673AB7","size":[55,55],"position":[0,0]},{"id":135,"properties":{"id":"Contribution_Justification","label":"__Entity__:Section"},"color":"#CDDC39","styles":{},"label":"Contribution_Justification","scale_factor":1,"type":"#CDDC39","size":[55,55],"position":[0,0]},{"id":136,"properties":{"id":"Efforts_Outcomes_Table","label":"__Entity__:Table"},"color":"#9E9E9E","styles":{},"label":"Efforts_Outcomes_Table","scale_factor":1,"type":"#9E9E9E","size":[55,55],"position":[0,0]},{"id":137,"properties":{"id":"Aspects_Contribution","label":"__Entity__:Aspect"},"color":"#9C27B0","styles":{},"label":"Aspects_Contribution","scale_factor":1,"type":"#9C27B0","size":[55,55],"position":[0,0]},{"id":138,"properties":{"id":"Ml_Methods_Architecture","label":"__Entity__:Aspect"},"color":"#9C27B0","styles":{},"label":"Ml_Methods_Architecture","scale_factor":1,"type":"#9C27B0","size":[55,55],"position":[0,0]},{"id":139,"properties":{"id":"850f1b5794916bd75706a90f7a23fbfa","text":" ensemble learning by straighRorwardly integraDng mulDple ML models Level 2-3: Design innovaDve architecture or pipeline or adapt ML methods, which have changed the way how learning and predicDon will be conducted.  AdopDng methods proposed by high-quality research papers is also encouraged.  Crea)vity and insights in understanding or further explaining the predic)on results and performance, in iden)fying the bias of machine learning output, the gap or confusion between ML outcome and business decision making  Level 1: describe results and explain the model performance without extra analysis or without further insights ","source":"Without Table BT4222 Project Deliverable Guideline 2024-25 Term1.pdf","page":2,"label":"Document"},"color":"#2196F3","styles":{},"label":"850f1b5794916bd75706a90f7a23fbfa","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":140,"properties":{"id":"Ensemble Learning","label":"__Entity__:Machine learning"},"color":"#2196F3","styles":{},"label":"Ensemble Learning","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]},{"id":141,"properties":{"id":"Ml Models","label":"__Entity__:Machine learning"},"color":"#2196F3","styles":{},"label":"Ml Models","scale_factor":1,"type":"#2196F3","size":[55,55],"position":[0,0]}],"_overview":{"enabled":null,"overview_set":false},"_selected_graph":[[],[]],"_sidebar":{"enabled":false,"start_with":null},"_view_count":null,"_view_module":"yfiles-jupyter-graphs","_view_module_version":"^1.8.1","_view_name":"GraphView","layout":"IPY_MODEL_000d3bc843914fb59126cde96a6df381","tabbable":null,"tooltip":null}},"000d3bc843914fb59126cde96a6df381":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"760px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}}}}},"nbformat":4,"nbformat_minor":0}